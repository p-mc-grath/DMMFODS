{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Basic Setup\n",
        "### 1. Installation\n",
        "* connect to own drive\n",
        "* create paths used throughout the notebook\n",
        "* get and install own repo\n",
        "* get and install waymo repo\n",
        "\n",
        "### 2. Data\n",
        "* transfer waymo dataset from gcs to gdrive\n",
        "* unpack \n",
        "* convert \n",
        "\n",
        "### 3. Training\n",
        "* copy list of files into data dir if necessary\n",
        "* training loop\n",
        "* visual assessment\n",
        "\n",
        "Remarks:\n",
        "* __linux_version paths \n",
        "    * should NOT be concatenated using e.g. os.path.join\n",
        "    * for usage with magic command: {path}\n",
        "* waymo: tf is version 1.x\n",
        "* data transfer: \n",
        "    * runtime cpu\n",
        "    * google file stream only allows a limited number of operations per time interval  \n",
        "* training: runtime gpu\n",
        "* tensorboard: enable 3rd party cookies in your browser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jEd97389k_4_"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x \n",
        "\n",
        "'''\n",
        "MOUNT\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Create PATHS\n",
        "'''\n",
        "\n",
        "ROOT_DIR__linux_version = '/content/drive/My\\ Drive/Colab\\ Notebooks/DeepCV_Packages/'\n",
        "DATA_DIR__linux_version = ROOT_DIR__linux_version + 'data/'\n",
        "REPO_DIR__linux_version = ROOT_DIR__linux_version + 'DeepCVLab/'\n",
        "DEEPCVLAB_DIR__linux_version = REPO_DIR__linux_version + 'deepcvlab/'\n",
        "\n",
        "ARCHIVE_DEST_DIR__linux_version = '/content/drive/My\\ Drive/Colab\\ Notebooks/'                  # this should be a repo containing very few files\n",
        "\n",
        "newly_cloned = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "GET OWN REPO\n",
        "'''\n",
        "\n",
        "%cd {ROOT_DIR__linux_version}\n",
        "!rm -rf {REPO_DIR__linux_version}\n",
        "!git clone https://github.com/pmcgrath249/DeepCVLab.git\n",
        "\n",
        "newly_cloned = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YrDqTMowdI3a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "INSTALL EVERYTHING\n",
        "'''\n",
        "\n",
        "# permanently change dir \n",
        "%cd {DEEPCVLAB_DIR__linux_version}\n",
        "\n",
        "# install waymo dataset utils in utils; https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial.ipynb\n",
        "!cd utils && rm -rf waymo-od > /dev/null\n",
        "!cd utils && git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!cd utils/waymo-od && git branch -a\n",
        "!cd utils/waymo-od && git checkout remotes/origin/r1.0\n",
        "!pip3 install --upgrade pip\n",
        "!pip3 install waymo-open-dataset\n",
        "\n",
        "# install requirements\n",
        "!cd {REPO_DIR__linux_version} && pip3 install -r requirements.txt\n",
        "\n",
        "# install own package\n",
        "!cd {REPO_DIR__linux_version} && python3 -m pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. DATA\n",
        "\n",
        "Data source: https://console.cloud.google.com/storage/browser/waymo_open_dataset_v_1_0_0\n",
        "\n",
        "### Transfer\n",
        "\n",
        "Help 1: https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041\n",
        "\n",
        "I had to change Help 1 because I was not able to find  the project_id nessecary for this approach\n",
        "\n",
        "Help 2: https://cloud.google.com/storage/docs/access-public-data?hl=de \n",
        "\n",
        "REMARK: No costs arise as the bucket is managed by waymo\n",
        "\n",
        "### Note\n",
        "\n",
        "* Use a CPU: runtime for this section. It gives access to more disk storage w.r.t. the compute instance. Unpacking clutters the disk.\n",
        "* No os.join for __linux_version paths: cannot be concatenated using os.join because of spaces and escaping characters within the paths\n",
        "* Small number of files/dir: Due to COLABxDRIVE issues, it is important to copy datasets to directories with little content. That's why data is divided into unnecessary subdirectories\n",
        "Otherwise it is not possible to extract files from the archives reliably.\n",
        "Moreover, I have had issues with moving archives -> iterative procedure  \n",
        "https://research.google.com/colaboratory/faq.html#drive-timeout   \n",
        "* Serialize in batches: Loading data from Drive is the bottleneck at train time. I found that disks cannot keep up with a high number of operations, i.e. loading batched data can speed up the loading procedure by 3-10x\n",
        "* only a limited number of operations is allowed regarding google file stream (gdrive & colab) -> if too many operations are requested in a certain amount of time, you will not be able to run any notebooks for 24hrs \n",
        "    * colab disk is going to fill\n",
        "    * your notebook fails to save\n",
        "    * although operations are being processed nothing is written to gdrive\n",
        "* Empty the trash once in a while to not run in trouble with unpacking archives\n",
        "  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "AUTHENTICATE GCS\n",
        "'''\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "LIST DIR TO BE COPIED\n",
        "'''\n",
        "\n",
        "bucket_name = 'waymo_open_dataset_v_1_0_0'\n",
        "!gsutil ls -r gs://{bucket_name}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "COPY\n",
        "UNPACK\n",
        "REDISTRIBUTE\n",
        "CONVERT\n",
        "BATCHING\n",
        "REDISTRIBUTE\n",
        "'''\n",
        "\n",
        "import os\n",
        "from deepcvlab.utils import Dense_U_Net_lidar_helper as utils\n",
        "from pathlib import Path\n",
        "\n",
        "config = utils.get_config()\n",
        "\n",
        "# naming\n",
        "bucket_name = 'waymo_open_dataset_v_1_0_0'\n",
        "training_bucket = os.path.join(bucket_name, 'training')\n",
        "\n",
        "for i in range(4):                                                                                 # from ls above\n",
        "    dataset_name = 'training_000{}.tar'.format(i) if i < 10 else 'training_00{}.tar'.format(i)      # right amount of leading zeros\n",
        "    data_bucket = os.path.join(training_bucket, dataset_name)                 \n",
        "    current_training_dir = os.path.join(config.dir.data.root, dataset_name[:-4])\n",
        "\n",
        "    # copy\n",
        "    print('start copying: ' + dataset_name)\n",
        "    Bucket_Dest__linux_version = DATA_DIR__linux_version + dataset_name[:-4] + '/'\n",
        "    !mkdir -p {Bucket_Dest__linux_version}\n",
        "    !gsutil -m cp -r gs://{data_bucket}/ {Bucket_Dest__linux_version}                               # copy multi-threaded and recursively\n",
        "\n",
        "    # unpack\n",
        "    print('start unpacking: ' + dataset_name)\n",
        "    archive_full_path = Bucket_Dest__linux_version + dataset_name\n",
        "    !tar -xvf {archive_full_path} -C {Bucket_Dest__linux_version}\n",
        "\n",
        "    print('deleting archive: ' + dataset_name)\n",
        "    !rm {archive_full_path}\n",
        "\n",
        "    filenames = os.listdir(current_training_dir)\n",
        "\n",
        "    # redistribute\n",
        "    for j, filename in enumerate(filenames):\n",
        "        if not filename.endswith('tfrecord'):\n",
        "            continue\n",
        "        tf_data_dir = 'tf_' + str(j)\n",
        "        Path(os.path.join(current_training_dir, tf_data_dir)).mkdir()\n",
        "        oldpath = os.path.join(current_training_dir, filename)\n",
        "        newpath = os.path.join(current_training_dir, tf_data_dir, filename)\n",
        "        os.rename(oldpath, newpath)\n",
        "\n",
        "    # convert\n",
        "    print('converting data of dir: ' + str(i))\n",
        "    utils.waymo_to_pytorch_offline(data_root=current_training_dir, idx_dataset_batch=i)\n",
        "\n",
        "    # batching and redistributing into train, val, test\n",
        "    print('batching data of dir: ' + str(i) + ' and putting it into ' + mode)\n",
        "    config = utils.get_config()\n",
        "    mode = 'train' \n",
        "    utils.save_data_in_batch(config, dataset_name, mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "TRAINING WITH TENSORBOARD VISUALIZATION\n",
        "'''\n",
        "\n",
        "# import\n",
        "%cd {REPO_DIR__linux_version}\n",
        "from deepcvlab.utils.Dense_U_Net_lidar_helper import get_config\n",
        "from deepcvlab.agents.Dense_U_Net_lidar_Agent import Dense_U_Net_lidar_Agent as Dense_U_Agent\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "config = get_config()\n",
        "config.agent.max_epoch = 20\n",
        "\n",
        "# agent takes care of everything incl. tensorboard dirs\n",
        "agent = Dense_U_Agent(config=config, torchvision_init=True)\n",
        "\n",
        "# use tensorboard to visualize\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {config.dir.summary}\n",
        "\n",
        "# if newly installed\n",
        "if newly_cloned:\n",
        "    Path(config.dir.data.file_lists).mkdir(exist_ok=True)\n",
        "    !cp {ROOT_DIR__linux_version + config.dataset.file_list_name} {DEEPCVLAB_DIR__linux_version + 'data/' + config.dataset.file_list_name} \n",
        "\n",
        "# start training\n",
        "agent.run()\n",
        "agent.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "VISUALLY ASSESS DATA AFTER FORWARD PASS\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from deepcvlab.utils.Dense_U_Net_lidar_helper import get_config\n",
        "from deepcvlab.agents.Dense_U_Net_lidar_Agent import Dense_U_Net_lidar_Agent as Dense_U_Agent\n",
        "\n",
        "def visual_assessment(img, lidar, pred, gt):\n",
        "\n",
        "    num_plots = gt.shape[0]\n",
        "    fig=plt.figure(figsize=(4*7,num_plots*7))\n",
        "    for i in range(num_plots):\n",
        "        \n",
        "        # rgb image\n",
        "        im = img[i].permute(1, 2, 0).detach().numpy().astype(np.uint8)\n",
        "        fig.add_subplot(num_plots, 4, i*4+1)   \n",
        "        plt.imshow(im)\n",
        "\n",
        "        # lidar image\n",
        "        l = lidar[i].permute(1, 2, 0)[:,:,0].detach().numpy().astype(np.uint8)\n",
        "        fig.add_subplot(num_plots, 4, i*4+2)   \n",
        "        plt.imshow(l, cmap=plt.cm.gray)\n",
        "\n",
        "        # network output\n",
        "        p = pred[i].permute(1, 2, 0)[:,:,0].detach().numpy().astype(np.uint8)\n",
        "        fig.add_subplot(num_plots, 4, i*4+3)   \n",
        "        plt.imshow(p, cmap=plt.cm.gray)\n",
        "\n",
        "        # ground truth\n",
        "        g = gt[i].permute(1, 2, 0)[:,:,0].detach().numpy()\n",
        "        fig.add_subplot(num_plots, 4, i*4+4)   \n",
        "        plt.imshow(g, cmap=plt.cm.gray)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "config = get_config()\n",
        "config.optimizer.mode = 'train'\n",
        "agent = Dense_U_Agent(torchvision_init=True)\n",
        "\n",
        "# visualize one batch\n",
        "for image, lidar, ht_map in agent.data_loader.train_loader:\n",
        "    if agent.cuda:\n",
        "        image = image.cuda()\n",
        "        lidar = lidar.cuda()\n",
        "    prediction = agent.model(image, lidar)\n",
        "    visual_assessment(image.cpu(), lidar.cpu(), prediction.cpu(), ht_map)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab_Setup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}